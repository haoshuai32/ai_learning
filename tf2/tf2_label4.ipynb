{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 构建卷积神经网络 (CNN) 以增强计算机视觉\n",
    "\n",
    "**学习内容**\n",
    "- 如何使用卷积提高计算机视觉和准确率\n",
    "\n",
    "**构建内容**\n",
    "- 用于增强神经网络的层"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. 使用卷积提高计算机视觉准确率\n",
    "您现在已知道如何使用深度神经网络 (DNN) 进行时尚图像识别，该网络包含三层：输入层（输入数据的形状）、输出层（理想输出的形状）和隐藏层。您使用数个影响最终准确率的参数（例如隐藏层的不同大小和训练周期数）进行了实验。\n",
    "\n",
    "为方便起见，我们重新在下面提供了完整的代码。运行该代码并记下最后输出的测试准确率。\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "1875/1875 [==============================] - 5s 3ms/step - loss: 0.5052 - accuracy: 0.8239\n",
      "Epoch 2/5\n",
      "1875/1875 [==============================] - 6s 3ms/step - loss: 0.3805 - accuracy: 0.8633\n",
      "Epoch 3/5\n",
      "1875/1875 [==============================] - 5s 2ms/step - loss: 0.3425 - accuracy: 0.8751\n",
      "Epoch 4/5\n",
      "1875/1875 [==============================] - 5s 3ms/step - loss: 0.3161 - accuracy: 0.8838\n",
      "Epoch 5/5\n",
      "1875/1875 [==============================] - 5s 3ms/step - loss: 0.2995 - accuracy: 0.8897\n",
      "313/313 [==============================] - 1s 2ms/step - loss: 0.3595 - accuracy: 0.8711\n",
      "Test loss: 0.3595237135887146, Test accuracy: 87.11000084877014\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "mnist = tf.keras.datasets.fashion_mnist\n",
    "(training_images, training_labels), (test_images, test_labels) = mnist.load_data()\n",
    "training_images=training_images/255.0\n",
    "test_images=test_images/255.0\n",
    "model = tf.keras.models.Sequential([\n",
    "  tf.keras.layers.Flatten(),\n",
    "  tf.keras.layers.Dense(128, activation='relu'),\n",
    "  tf.keras.layers.Dense(10, activation='softmax')\n",
    "])\n",
    "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "model.fit(training_images, training_labels, epochs=5)\n",
    "test_loss, test_accuracy = model.evaluate(test_images, test_labels)\n",
    "print ('Test loss: {}, Test accuracy: {}'.format(test_loss, test_accuracy*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "您的训练准确率可能约为 89%，验证准确率约为 87%。使用卷积可以进一步提升准确率，卷积会缩小图像内容的范围，以便专注于特定的细节。\n",
    "\n",
    "如果您曾使用[过滤器进行图像处理](tf2_label3.ipynb)，则卷积看起来会非常熟悉。\n",
    "\n",
    "简而言之，您需要一个数组（通常为 3x3 或 5x5）并将其传递给图像。通过基于矩阵中的公式更改底层像素，您可以执行边缘检测等操作。例如，通常定义一个 3x3 数组用于边缘检测，其中中间单元格为 8，并且其所有相邻单元格均为 -1。在本例中，您需将每个像素的值乘以 8，然后减去每个相邻像素的值。为每个像素执行上述操作，最终得到一个边缘已增强的新图像。\n",
    "\n",
    "这非常适合计算机视觉，因为增强边缘等特征有助于计算机区分不同事物。更好的是，所需的信息量会少得多，因为您只要针对突出特征进行训练。\n",
    "\n",
    "这就是卷积神经网络的概念。在生成密集层之前添加一些层进行卷积，然后到达密集层的信息会变得更加集中，并且可能更加准确。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. 试用代码\n",
    "运行以下代码。它与之前的神经网络相同，但这次首先添加了卷积层。这将需要更长的时间，但请观察对准确率的影响：\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.11.0\n",
      "60000\n",
      "10000\n",
      "Model: \"sequential_2\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " conv2d_2 (Conv2D)           (None, 26, 26, 64)        640       \n",
      "                                                                 \n",
      " max_pooling2d_2 (MaxPooling  (None, 13, 13, 64)       0         \n",
      " 2D)                                                             \n",
      "                                                                 \n",
      " conv2d_3 (Conv2D)           (None, 11, 11, 64)        36928     \n",
      "                                                                 \n",
      " max_pooling2d_3 (MaxPooling  (None, 5, 5, 64)         0         \n",
      " 2D)                                                             \n",
      "                                                                 \n",
      " flatten_2 (Flatten)         (None, 1600)              0         \n",
      "                                                                 \n",
      " dense_4 (Dense)             (None, 128)               204928    \n",
      "                                                                 \n",
      " dense_5 (Dense)             (None, 10)                1290      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 243,786\n",
      "Trainable params: 243,786\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/5\n",
      "1875/1875 [==============================] - 51s 26ms/step - loss: 0.4307 - accuracy: 0.8431\n",
      "Epoch 2/5\n",
      "1875/1875 [==============================] - 49s 26ms/step - loss: 0.2884 - accuracy: 0.8930\n",
      "Epoch 3/5\n",
      "1875/1875 [==============================] - 48s 26ms/step - loss: 0.2447 - accuracy: 0.9104\n",
      "Epoch 4/5\n",
      "1875/1875 [==============================] - 49s 26ms/step - loss: 0.2128 - accuracy: 0.9215\n",
      "Epoch 5/5\n",
      "1845/1875 [============================>.] - ETA: 0s - loss: 0.1858 - accuracy: 0.9309"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "print(tf.__version__)\n",
    "mnist = tf.keras.datasets.fashion_mnist\n",
    "(training_images, training_labels), (test_images, test_labels) = mnist.load_data()\n",
    "training_images=training_images.reshape(60000, 28, 28, 1)\n",
    "\n",
    "print(len(training_images))\n",
    "\n",
    "training_images=training_images / 255.0\n",
    "test_images = test_images.reshape(10000, 28, 28, 1)\n",
    "\n",
    "print(len(test_images))\n",
    "\n",
    "test_images=test_images / 255.0\n",
    "model = tf.keras.models.Sequential([\n",
    "  tf.keras.layers.Conv2D(64, (3, 3), activation='relu', input_shape=(28, 28, 1)),\n",
    "  tf.keras.layers.MaxPooling2D(2, 2),\n",
    "  tf.keras.layers.Conv2D(64, (3, 3), activation='relu'),\n",
    "  tf.keras.layers.MaxPooling2D(2,2),\n",
    "  tf.keras.layers.Flatten(),\n",
    "  tf.keras.layers.Dense(128, activation='relu'),\n",
    "  tf.keras.layers.Dense(10, activation='softmax')\n",
    "])\n",
    "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "model.summary()\n",
    "model.fit(training_images, training_labels, epochs=5)\n",
    "test_loss, test_accuracy = model.evaluate(test_images, test_labels)\n",
    "print ('Test loss: {}, Test accuracy: {}'.format(test_loss, test_accuracy*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "训练数据准确率可能提升到约 93%，验证数据准确率可能提升到约 91%。\n",
    "\n",
    "现在，请尝试将其运行更多周期（例如大约 20 个周期）并探索其结果。虽然训练结果可能看起来非常好，但验证结果实际上可能会因为被称为过拟合的现象而导致效果下降。\n",
    "\n",
    "如果网络从训练集学习数据表现太好，就会发生过拟合，它会仅专注于识别这样的数据，因此在更普遍的情况下学习其他数据时效果就会不佳。例如，如果您针对大量高跟鞋数据进行训练，那么该网络可能非常擅长识别高跟鞋，但对于运动鞋可能就会发生混乱。\n",
    "\n",
    "再次查看代码，并逐步了解如何构建卷积。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. 收集数据\n",
    "第一步是收集数据。\n",
    "\n",
    "您会发现这里发生了一些变化，并且需要对训练数据进行改造。这是因为第一个卷积需要一个包含所有内容的张量，因此，您只有一个 60000x28x28x1 的 4D 列表，而不是在列表中包含 60000 个 28x28x1 项，测试图像也是如此。否则，您在训练时将收到错误消息，因为卷积无法识别其形状。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "mnist = tf.keras.datasets.fashion_mnist\n",
    "(training_images, training_labels), (test_images, test_labels) = mnist.load_data()\n",
    "training_images=training_images.reshape(60000, 28, 28, 1)\n",
    "training_images = training_images/255.0\n",
    "test_images = test_images.reshape(10000, 28, 28, 1)\n",
    "test_images = test_images/255.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. 定义模型\n",
    "接下来，定义您的模型。您将添加卷积层，而不是顶部的输入层。具体参数如下：\n",
    "\n",
    "您要生成的卷积数。诸如 32 之类的值是不错的起点。\n",
    "卷积矩阵的大小，在本例中为 3x3 网格。\n",
    "需要使用的激活函数，在本例中使用 relu。\n",
    "在第一层中，输入数据的形状。\n",
    "您将使用最大池化层紧接卷积，该层旨在压缩图像，同时保留由卷积突出显示的特征内容。为最大池化指定 (2,2)，结果会将图像的大小减至四分之一。它会创建一个 2x2 像素数组，并选择最大的像素值，从而将 4 个像素转换为 1 个像素。它会在整个图像中重复此计算，其结果是将水平像素数和垂直像素数均减半。\n",
    "\n",
    "您可以调用 model.summary() 查看网络的大小和形状。请注意，在每个最大池化层之后，图像大小都会通过以下方式减小：\n",
    "\n",
    "```\n",
    "_________________________________________________________________\n",
    "Layer (type)                 Output Shape              Param #\n",
    "=================================================================\n",
    "conv2d_2 (Conv2D)            (None, 26, 26, 64)        640\n",
    "_________________________________________________________________\n",
    "max_pooling2d_2 (MaxPooling2 (None, 13, 13, 64)        0\n",
    "_________________________________________________________________\n",
    "conv2d_3 (Conv2D)            (None, 11, 11, 64)        36928\n",
    "_________________________________________________________________\n",
    "max_pooling2d_3 (MaxPooling2 (None, 5, 5, 64)          0\n",
    "_________________________________________________________________\n",
    "flatten_2 (Flatten)          (None, 1600)              0\n",
    "_________________________________________________________________\n",
    "dense_4 (Dense)              (None, 128)               204928\n",
    "_________________________________________________________________\n",
    "dense_5 (Dense)              (None, 10)                1290\n",
    "=================================================================\n",
    "```\n",
    "\n",
    "以下是 CNN 的完整代码："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "model = tf.keras.models.Sequential([\n",
    "tf.keras.layers.Conv2D(32, (3, 3), activation='relu', input_shape=(28, 28, 1)),\n",
    "tf.keras.layers.MaxPooling2D(2, 2),\n",
    "#Add another convolution\n",
    "tf.keras.layers.Conv2D(64, (3,3), activation='relu'),\n",
    "tf.keras.layers.MaxPooling2D(2, 2),\n",
    "#Now flatten the output. After this you'll just have the same DNN structure as the non convolutional version\n",
    "tf.keras.layers.Flatten(),\n",
    "#The same 128 dense layers, and 10 output layers as in the pre-convolution example:\n",
    "tf.keras.layers.Dense(128, activation='relu'),\n",
    "tf.keras.layers.Dense(10, activation='softmax')\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. 编译和训练模型\n",
    "\n",
    "编译模型，调用拟合方法进行训练，然后评估测试集的损失和准确率。\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "model.fit(training_images, training_labels, epochs=5)\n",
    "test_loss, test_acc = model.evaluate(test_images, test_labels)\n",
    "print ('Test loss: {}, Test accuracy: {}'.format(test_loss, test_acc*100))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. 直观呈现卷积和池化\n",
    "此代码以图形方式显示卷积。print (test_labels[:100]) 显示了测试集内的前 100 个标签，可以看到索引 0、索引 23 和索引 28 处的标签都是相同的值 (9)。它们都是鞋子。查看对各种鞋运行卷积的结果，您会开始发现它们之间出现一些共同特征。现在，DNN 在基于该数据进行训练时，处理的信息会少得多，并且它可能根据这种卷积和池化组合找到鞋子之间的共同性。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(test_labels[:100])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "[9 2 1 1 6 1 4 6 5 7 4 5 7 3 4 1 2 4 8 0 2 5 7 9 1 4 6 0 9 3 8 8 3 3 8 0 7\n",
    " 5 7 9 6 1 3 7 6 7 2 1 2 2 4 4 5 8 2 2 8 4 8 0 7 7 8 5 1 1 2 3 9 8 7 0 2 6\n",
    " 2 3 1 2 8 4 1 8 5 9 5 0 3 2 0 6 5 3 6 7 1 8 0 1 4 2]\n",
    "```\n",
    "\n",
    "现在，您可以为这些标签选择一些相应的图像，并呈现它们在卷积过程中的显示效果。因此，在以下代码中，FIRST_IMAGE、SECOND_IMAGE 和 THIRD_IMAGE 都是值 9（踝靴）的索引。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-c034ab857944>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeras\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mmodels\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m \u001b[0mlayer_outputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mlayer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutput\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mlayer\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m \u001b[0mactivation_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mModel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlayer_outputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'model' is not defined"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 12 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "f, axarr = plt.subplots(3,4)\n",
    "FIRST_IMAGE=0\n",
    "SECOND_IMAGE=23\n",
    "THIRD_IMAGE=28\n",
    "CONVOLUTION_NUMBER = 6\n",
    "\n",
    "from tensorflow.keras import models\n",
    "\n",
    "layer_outputs = [layer.output for layer in model.layers]\n",
    "activation_model = tf.keras.models.Model(inputs = model.input, outputs = layer_outputs)\n",
    "for x in range(0,4):\n",
    "  f1 = activation_model.predict(test_images[FIRST_IMAGE].reshape(1, 28, 28, 1))[x]\n",
    "  axarr[0,x].imshow(f1[0, : , :, CONVOLUTION_NUMBER], cmap='inferno')\n",
    "  axarr[0,x].grid(False)\n",
    "  f2 = activation_model.predict(test_images[SECOND_IMAGE].reshape(1, 28, 28, 1))[x]\n",
    "  axarr[1,x].imshow(f2[0, : , :, CONVOLUTION_NUMBER], cmap='inferno')\n",
    "  axarr[1,x].grid(False)\n",
    "  f3 = activation_model.predict(test_images[THIRD_IMAGE].reshape(1, 28, 28, 1))[x]\n",
    "  axarr[2,x].imshow(f3[0, : , :, CONVOLUTION_NUMBER], cmap='inferno')\n",
    "  axarr[2,x].grid(False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "您应该能看到如下所示的内容，其中卷积获取鞋底的本质特征，实际上是将其看作所有鞋子的共同特征。\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "8. 练习\n",
    "练习 1\n",
    "尝试修改卷积。将卷积数从 32 更改为 16 或 64。这对准确率和训练时间有何影响？\n",
    "\n",
    "练习 2\n",
    "移除最终卷积。这对准确率或训练时间有何影响？\n",
    "\n",
    "练习 3\n",
    "添加更多卷积。这会带来什么影响？\n",
    "\n",
    "练习 4\n",
    "移除第一个卷积外的所有卷积。这会带来什么影响？对此进行实验。\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. 恭喜\n",
    "\n",
    "您已构建了首个 CNN！"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Lab4-Using-Convolutions.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.15"
  },
  "vscode": {
   "interpreter": {
    "hash": "f08c46d59517fdcee7a61744cb8d4e0f1d2ec7195fb58a0d049a3a7590653ebd"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
