{
  "cells": [
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# 使用卷积神经网络 (CNN) 处理复杂图像\n",
        "\n",
        "**学习内容**\n",
        "\n",
        " - 如何训练计算机识别图像中不清晰物体的特征\n",
        "\n",
        "**您将构建的内容**\n",
        " - 卷积神经网络，可区分马和人的照片"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. 使用入门：获取数据\n",
        "\n",
        "\n",
        "您可以通过构建一个 horses-or-humans 分类器实现这一功能，用于识别给定图像是否包含马或人，您需要训练此网络识别马与人的特征。您必须先对数据做一些处理，然后才能进行训练。\n",
        "\n",
        "首先，下载数据"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "vscode": {
          "languageId": "plaintext"
        }
      },
      "outputs": [],
      "source": [
        "!wget --no-check-certificate https://storage.googleapis.com/laurencemoroney-blog.appspot.com/horse-or-human.zip  -O /home/haoshuai/code/dataset/horse-or-human.zip"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "以下 Python 代码将导入 OS 库来使用各操作系统库，使您能够访问文件系统和 ZipFile 库，以便解压缩数据。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "vscode": {
          "languageId": "plaintext"
        }
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import zipfile\n",
        "\n",
        "local_zip = '/tmp/horse-or-human.zip'\n",
        "zip_ref = zipfile.ZipFile(local_zip, 'r')\n",
        "zip_ref.extractall('/tmp/horse-or-human')\n",
        "zip_ref.close()"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "该 ZIP 文件的内容将解压缩到包含马和人子目录的基础目录 /home/haoshuai/code/dataset/horse-or-human 中。\n",
        "\n",
        "简而言之，训练集中包含的数据会告诉神经网络模型“这是马的样子”和“这是人的样子”。"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. 使用 ImageGenerator 标记和准备数据\n",
        "\n",
        "您并不需要明确地将图像标记为马或人。\n",
        "\n",
        "稍后，您会看到系统使用了名为 `ImageDataGenerator` 的程序。它会读取子目录中的图像，并根据子目录的名称自动标记这些图像。例如，您的训练目录包含一个马目录和一个人目录。`ImageDataGenerator` 会为图像添加合适的标签，从而减少编码步骤。\n",
        "\n",
        "定义每个目录。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "vscode": {
          "languageId": "plaintext"
        }
      },
      "outputs": [],
      "source": [
        "# Directory with our training horse pictures\n",
        "train_horse_dir = os.path.join('/home/haoshuai/code/dataset/horse-or-human/horses')\n",
        "\n",
        "# Directory with our training human pictures\n",
        "train_human_dir = os.path.join('/home/haoshuai/code/dataset/horse-or-human/humans')"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "现在，查看马和人训练目录中的文件名是什么样子："
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "vscode": {
          "languageId": "plaintext"
        }
      },
      "outputs": [],
      "source": [
        "train_horse_names = os.listdir(train_horse_dir)\n",
        "print(train_horse_names[:10])\n",
        "train_human_names = os.listdir(train_human_dir)\n",
        "print(train_human_names[:10])"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "查看目录中马和人的图像总数："
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "vscode": {
          "languageId": "plaintext"
        }
      },
      "outputs": [],
      "source": [
        "print('total training horse images:', len(os.listdir(train_horse_dir)))\n",
        "print('total training human images:', len(os.listdir(train_human_dir)))"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. 探索数据\n",
        "\n",
        "查看几张照片，更好地了解它们的样子。\n",
        "\n",
        "首先，配置 `matplot` 参数："
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "vscode": {
          "languageId": "plaintext"
        }
      },
      "outputs": [],
      "source": [
        "%matplotlib inline\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.image as mpimg\n",
        "\n",
        "# Parameters for our graph; we'll output images in a 4x4 configuration\n",
        "nrows = 4\n",
        "ncols = 4\n",
        "\n",
        "# Index for iterating over images\n",
        "pic_index = 0"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "现在，批量显示 8 张马的照片和 8 张人的照片。您可以每次重新运行单元格，查看新的批次数据。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "vscode": {
          "languageId": "plaintext"
        }
      },
      "outputs": [],
      "source": [
        "# Set up matplotlib fig, and size it to fit 4x4 pics\n",
        "fig = plt.gcf()\n",
        "fig.set_size_inches(ncols * 4, nrows * 4)\n",
        "\n",
        "pic_index += 8\n",
        "next_horse_pix = [os.path.join(train_horse_dir, fname)\n",
        "                for fname in train_horse_names[pic_index-8:pic_index]]\n",
        "next_human_pix = [os.path.join(train_human_dir, fname)\n",
        "                for fname in train_human_names[pic_index-8:pic_index]]\n",
        "\n",
        "for i, img_path in enumerate(next_horse_pix+next_human_pix):\n",
        "  # Set up subplot; subplot indices start at 1\n",
        "  sp = plt.subplot(nrows, ncols, i + 1)\n",
        "  sp.axis('Off') # Don't show axes (or gridlines)\n",
        "\n",
        "  img = mpimg.imread(img_path)\n",
        "  plt.imshow(img)\n",
        "\n",
        "plt.show()"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "下面是一些展示马和人不同姿势及方向的示例图像：\n",
        "\n",
        "![image0](/images/6b6ebbc6e694ccd2.png)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5. 定义模型\n",
        "\n",
        "开始定义模型。\n",
        "\n",
        "首先导入 TensorFlow：\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "vscode": {
          "languageId": "plaintext"
        }
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "然后，添加卷积层并扁平化最终结果，将其馈送到密集连接层。最后，添加密集连接层。\n",
        "\n",
        "注意：由于您面对的是两类分类问题（即，二元分类问题），您的网络最终会以 sigmoid 激活函数结束，使得网络的输出为 0 到 1 之间的单个标量，表示当前图像为第 1 类（而非第 0 类）的概率。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "vscode": {
          "languageId": "plaintext"
        }
      },
      "outputs": [],
      "source": [
        "model = tf.keras.models.Sequential([\n",
        "    # Note the input shape is the desired size of the image 300x300 with 3 bytes color\n",
        "    # This is the first convolution\n",
        "    tf.keras.layers.Conv2D(16, (3,3), activation='relu', input_shape=(300, 300, 3)),\n",
        "    tf.keras.layers.MaxPooling2D(2, 2),\n",
        "    # The second convolution\n",
        "    tf.keras.layers.Conv2D(32, (3,3), activation='relu'),\n",
        "    tf.keras.layers.MaxPooling2D(2,2),\n",
        "    # The third convolution\n",
        "    tf.keras.layers.Conv2D(64, (3,3), activation='relu'),\n",
        "    tf.keras.layers.MaxPooling2D(2,2),\n",
        "    # The fourth convolution\n",
        "    tf.keras.layers.Conv2D(64, (3,3), activation='relu'),\n",
        "    tf.keras.layers.MaxPooling2D(2,2),\n",
        "    # The fifth convolution\n",
        "    tf.keras.layers.Conv2D(64, (3,3), activation='relu'),\n",
        "    tf.keras.layers.MaxPooling2D(2,2),\n",
        "    # Flatten the results to feed into a DNN\n",
        "    tf.keras.layers.Flatten(),\n",
        "    # 512 neuron hidden layer\n",
        "    tf.keras.layers.Dense(512, activation='relu'),\n",
        "    # Only 1 output neuron. It will contain a value from 0-1 where 0 for 1 class ('horses') and 1 for the other ('humans')\n",
        "    tf.keras.layers.Dense(1, activation='sigmoid')\n",
        "])"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "`model.summary()` 方法调用会输出网络的摘要。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "vscode": {
          "languageId": "plaintext"
        }
      },
      "outputs": [],
      "source": [
        "model.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "vscode": {
          "languageId": "plaintext"
        }
      },
      "outputs": [],
      "source": [
        "您会看到如下所示的结果：\n",
        "\n",
        "```sh\n",
        "Layer (type)                 Output Shape              Param #\n",
        "=================================================================\n",
        "conv2d (Conv2D)              (None, 298, 298, 16)      448\n",
        "_________________________________________________________________\n",
        "max_pooling2d (MaxPooling2D) (None, 149, 149, 16)      0\n",
        "_________________________________________________________________\n",
        "conv2d_1 (Conv2D)            (None, 147, 147, 32)      4640\n",
        "_________________________________________________________________\n",
        "max_pooling2d_1 (MaxPooling2 (None, 73, 73, 32)        0\n",
        "_________________________________________________________________\n",
        "conv2d_2 (Conv2D)            (None, 71, 71, 64)        18496\n",
        "_________________________________________________________________\n",
        "max_pooling2d_2 (MaxPooling2 (None, 35, 35, 64)        0\n",
        "_________________________________________________________________\n",
        "conv2d_3 (Conv2D)            (None, 33, 33, 64)        36928\n",
        "_________________________________________________________________\n",
        "max_pooling2d_3 (MaxPooling2 (None, 16, 16, 64)        0\n",
        "_________________________________________________________________\n",
        "conv2d_4 (Conv2D)            (None, 14, 14, 64)        36928\n",
        "_________________________________________________________________\n",
        "max_pooling2d_4 (MaxPooling2 (None, 7, 7, 64)          0\n",
        "_________________________________________________________________\n",
        "flatten (Flatten)            (None, 3136)              0\n",
        "_________________________________________________________________\n",
        "dense (Dense)                (None, 512)               1606144\n",
        "_________________________________________________________________\n",
        "dense_1 (Dense)              (None, 1)                 513\n",
        "=================================================================\n",
        "Total params: 1,704,097\n",
        "Trainable params: 1,704,097\n",
        "Non-trainable params: 0\n",
        "```\n",
        "\n",
        "输出形状列显示了特征图的大小在每个连续层中的变化情况。由于填充操作，卷积层会稍微减小特征图的大小，而每个池化层则将特征图的大小减半。"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 6. 编译模型\n",
        "\n",
        "接下来，配置模型训练的规范。用 `binary_crossentropy` 损失函数训练模型，因为它是二元分类问题，而最终激活函数是 `sigmoid`。（若要复习损失指标，请参阅深入了解机器学习。）使用 `rmsprop` 优化器，学习速率为 0.001。在训练期间，监控分类准确率。\n",
        "\n",
        "\n",
        "> 注意：在这种情况下，使用 RMSprop 优化算法优于随机梯度下降法 (SGD)，因为 RMSprop 会为您自动调整学习速率。（Adam 和 Adagrad 等其他优化器也会在训练期间自动调整学习速率，它们在此 Codelab 中效果一样。）\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "vscode": {
          "languageId": "plaintext"
        }
      },
      "outputs": [],
      "source": [
        "from tensorflow.keras.optimizers import RMSprop\n",
        "\n",
        "model.compile(loss='binary_crossentropy',\n",
        "              optimizer=RMSprop(lr=0.001),\n",
        "              metrics=['acc'])"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 7. 从生成器训练模型\n",
        "设置数据生成器，用于读取源文件夹中的照片，将照片转换为 float32 张量，并将照片（及其标签）馈送到您的网络中。\n",
        "\n",
        "您将拥有一个用于训练图像的生成器，以及一个用于验证图像的生成器。您的生成器将批量生成大小为 300x300 的图像及其二元标签。\n",
        "\n",
        "您可能已经知道，进入神经网络的数据通常应该以某种方式归一化，以更易于网络处理。（向 CNN 提供原始像素的情况并不常见。）在此 Codelab 中，预处理图像的方式是：将像素值归一化到 [0, 1] 范围内（最初所有值都在 [0, 255] 范围内）。\n",
        "\n",
        "在 Keras 中，可通过使用重新缩放参数的 keras.preprocessing.image.ImageDataGenerator 类来实现此目的。借助 ImageDataGenerator 类，您可以通过 .flow(data, labels) 或 .flow_from_directory(directory) 实例化会生成增强批量图像（及其标签）的生成器。然后，您可以将这些生成器与接受数据生成器作为输入的 Keras 模型方法结合使用：fit_generator、evaluate_generator 和 predict_generator。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "vscode": {
          "languageId": "plaintext"
        }
      },
      "outputs": [],
      "source": [
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "\n",
        "# All images will be rescaled by 1./255\n",
        "train_datagen = ImageDataGenerator(rescale=1./255)\n",
        "\n",
        "# Flow training images in batches of 128 using train_datagen generator\n",
        "train_generator = train_datagen.flow_from_directory(\n",
        "        '/tmp/horse-or-human/',  # This is the source directory for training images\n",
        "        target_size=(300, 300),  # All images will be resized to 150x150\n",
        "        batch_size=128,\n",
        "        # Since we use binary_crossentropy loss, we need binary labels\n",
        "        class_mode='binary')"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 8. 进行训练\n",
        "训练 15 个周期。（可能需要几分钟的时间运行。）"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "vscode": {
          "languageId": "plaintext"
        }
      },
      "outputs": [],
      "source": [
        "history = model.fit(\n",
        "      train_generator,\n",
        "      steps_per_epoch=8,\n",
        "      epochs=15,\n",
        "      verbose=1)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "注意每个周期的值。\n",
        "\n",
        "损失和准确率是训练进度的重要标志。模型会猜测训练数据的分类情况，然后根据已知标签对其进行衡量并计算结果。准确率表示正确猜测所占的比例。\n",
        "\n",
        "\n",
        "```sh\n",
        "Epoch 1/15\n",
        "9/9 [==============================] - 9s 1s/step - loss: 0.8662 - acc: 0.5151\n",
        "Epoch 2/15\n",
        "9/9 [==============================] - 8s 927ms/step - loss: 0.7212 - acc: 0.5969\n",
        "Epoch 3/15\n",
        "9/9 [==============================] - 8s 921ms/step - loss: 0.6612 - acc: 0.6592\n",
        "Epoch 4/15\n",
        "9/9 [==============================] - 8s 925ms/step - loss: 0.3135 - acc: 0.8481\n",
        "Epoch 5/15\n",
        "9/9 [==============================] - 8s 919ms/step - loss: 0.4640 - acc: 0.8530\n",
        "Epoch 6/15\n",
        "9/9 [==============================] - 8s 896ms/step - loss: 0.2306 - acc: 0.9231\n",
        "Epoch 7/15\n",
        "9/9 [==============================] - 8s 915ms/step - loss: 0.1464 - acc: 0.9396\n",
        "Epoch 8/15\n",
        "9/9 [==============================] - 8s 935ms/step - loss: 0.2663 - acc: 0.8919\n",
        "Epoch 9/15\n",
        "9/9 [==============================] - 8s 883ms/step - loss: 0.0772 - acc: 0.9698\n",
        "Epoch 10/15\n",
        "9/9 [==============================] - 9s 951ms/step - loss: 0.0403 - acc: 0.9805\n",
        "Epoch 11/15\n",
        "9/9 [==============================] - 8s 891ms/step - loss: 0.2618 - acc: 0.9075\n",
        "Epoch 12/15\n",
        "9/9 [==============================] - 8s 902ms/step - loss: 0.0434 - acc: 0.9873\n",
        "Epoch 13/15\n",
        "9/9 [==============================] - 8s 904ms/step - loss: 0.0187 - acc: 0.9932\n",
        "Epoch 14/15\n",
        "9/9 [==============================] - 9s 951ms/step - loss: 0.0974 - acc: 0.9649\n",
        "Epoch 15/15\n",
        "9/9 [==============================] - 8s 877ms/step - loss: 0.2859 - acc: 0.9338\n",
        "```"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 9. 测试模型\n",
        "现在，使用该模型进行实际预测。下方的代码将使您能够从文件系统中选择一个或多个文件。然后，代码将上传这些文件并将文件传入模型中，最终指明对象是马还是人。\n",
        "\n",
        "您可以将互联网中的图像下载到文件系统中，试一试！请注意，尽管训练准确率高于 99%，但您可能会发现神经网络会犯许多错误。\n",
        "\n",
        "这是由于所谓的过拟合造成的，即神经网络使用非常有限的数据进行训练（每个类别只有大约 500 张图像）。因此，它非常擅长识别与训练集中的图像相似的图像，但对于训练集中不包含的图像，错误率会很高。\n",
        "\n",
        "这是一个数据点，证明您训练的数据越多，最终网络将会越出色！\n",
        "\n",
        "尽管数据有限，但有许多技术可以用来改善训练，包括图像增强技术，但这超出了此 Codelab 的讨论范围。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "vscode": {
          "languageId": "plaintext"
        }
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "from google.colab import files\n",
        "from keras.preprocessing import image\n",
        "\n",
        "uploaded = files.upload()\n",
        "\n",
        "for fn in uploaded.keys():\n",
        "\n",
        "  # predicting images\n",
        "  path = '/content/' + fn\n",
        "  img = image.load_img(path, target_size=(300, 300))\n",
        "  x = image.img_to_array(img)\n",
        "  x = np.expand_dims(x, axis=0)\n",
        "\n",
        "  images = np.vstack([x])\n",
        "  classes = model.predict(images, batch_size=10)\n",
        "  print(classes[0])\n",
        "  if classes[0]>0.5:\n",
        "    print(fn + \" is a human\")\n",
        "  else:\n",
        "    print(fn + \" is a horse\")"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "例如，假设您要使用以下图像进行测试：\n",
        "\n",
        "![image](images/9e07a57ff3be7a82.png)\n",
        "\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "尽管它是卡通图片，仍可以正确分类。\n",
        "\n",
        "下图也可以正确分类：\n",
        "\n",
        "![image](images/9e07a57ff3be7a82.png)\n",
        "\n",
        "尝试一些您自己的图像，一探究竟！"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 10. 直观呈现中间表示法\n",
        "如需感受 CNN 已经了解了哪些特征类型，有趣的做法是可视化输入在 CNN 中的转换方式。\n",
        "\n",
        "从训练集中选取随机图像，然后生成一个图表，其中每一行都是层的输出，而行中的每个图像都是该输出特征图中的特定过滤器。重新运行该单元格，以生成各种训练图像的中间表示法。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "vscode": {
          "languageId": "plaintext"
        }
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import random\n",
        "from tensorflow.keras.preprocessing.image import img_to_array, load_img\n",
        "\n",
        "# Let's define a new Model that will take an image as input, and will output\n",
        "# intermediate representations for all layers in the previous model after\n",
        "# the first.\n",
        "successive_outputs = [layer.output for layer in model.layers[1:]]\n",
        "#visualization_model = Model(img_input, successive_outputs)\n",
        "visualization_model = tf.keras.models.Model(inputs = model.input, outputs = successive_outputs)\n",
        "# Let's prepare a random input image from the training set.\n",
        "horse_img_files = [os.path.join(train_horse_dir, f) for f in train_horse_names]\n",
        "human_img_files = [os.path.join(train_human_dir, f) for f in train_human_names]\n",
        "img_path = random.choice(horse_img_files + human_img_files)\n",
        "\n",
        "img = load_img(img_path, target_size=(300, 300))  # this is a PIL image\n",
        "x = img_to_array(img)  # Numpy array with shape (150, 150, 3)\n",
        "x = x.reshape((1,) + x.shape)  # Numpy array with shape (1, 150, 150, 3)\n",
        "\n",
        "# Rescale by 1/255\n",
        "x /= 255\n",
        "\n",
        "# Let's run our image through our network, thus obtaining all\n",
        "# intermediate representations for this image.\n",
        "successive_feature_maps = visualization_model.predict(x)\n",
        "\n",
        "# These are the names of the layers, so can have them as part of our plot\n",
        "layer_names = [layer.name for layer in model.layers]\n",
        "\n",
        "# Now let's display our representations\n",
        "for layer_name, feature_map in zip(layer_names, successive_feature_maps):\n",
        "  if len(feature_map.shape) == 4:\n",
        "    # Just do this for the conv / maxpool layers, not the fully-connected layers\n",
        "    n_features = feature_map.shape[-1]  # number of features in feature map\n",
        "    # The feature map has shape (1, size, size, n_features)\n",
        "    size = feature_map.shape[1]\n",
        "    # We will tile our images in this matrix\n",
        "    display_grid = np.zeros((size, size * n_features))\n",
        "    for i in range(n_features):\n",
        "      # Postprocess the feature to make it visually palatable\n",
        "      x = feature_map[0, :, :, i]\n",
        "      x -= x.mean()\n",
        "      if x.std()>0:\n",
        "        x /= x.std()\n",
        "      x *= 64\n",
        "      x += 128\n",
        "      x = np.clip(x, 0, 255).astype('uint8')\n",
        "      # We'll tile each filter into this big horizontal grid\n",
        "      display_grid[:, i * size : (i + 1) * size] = x\n",
        "    # Display the grid\n",
        "    scale = 20. / n_features\n",
        "    plt.figure(figsize=(scale * n_features, scale))\n",
        "    plt.title(layer_name)\n",
        "    plt.grid(False)\n",
        "    plt.imshow(display_grid, aspect='auto', cmap='viridis')"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "结果示例如下：\n",
        "\n",
        "![image](images/e078d1bc9662c93f.png)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "如您所见，您从图像的原始像素过渡到越来越抽象且精简的表示法。随着学习的深入，表示法开始突出显示网络所关注的内容，且显示“激活”的特征越来越少。大多数都设置为 0。我们称之为“稀疏性”。表示法稀疏性是深度学习的主要功能。\n",
        "\n",
        "这些表示法包含的有关图像原始像素的信息日益减少，但关于图像类别的信息则日益精细。您可以将 CNN（或通常所说的深度网络）视为信息蒸馏管道。"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 11. 恭喜\n",
        "您了解了如何使用 CNN 增强复杂图像。"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "name": "Lab5-Using-Convolutions-With-Complex-Images.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "yolo_v5",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.7.15 (default, Nov 24 2022, 21:12:53) \n[GCC 11.2.0]"
    },
    "vscode": {
      "interpreter": {
        "hash": "8214cb87bc38f4c4e78397502388c3c0a5f5c0a9337d249b8bbbf697fb41cc8b"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
